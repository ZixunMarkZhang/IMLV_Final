# [Decision Tree]
In this section, we will create a decision tree model predicting y with all variables introduced in EDA. In the plot below, we generate a full decision tree where all the nodes are pure. However, it is obvious that the decision tree is over complicated and has the problem of overfitting. We may then plot the complexity plot then to prune the tree for making predictions. We may use the full models to visualize and interpret to ensure all variables are considered,
```{r,cache=TRUE}
set.seed(5293)
library(rpart)
library(rpart.plot)

DTmodf <- rpart(y ~ ., data = train_dat,cp=0,method='class')
pred <- function(model, newdata) {
  predict(model, newdata = newdata, type = "prob")[, 2]
}

rpart.plot(DTmodf)
```
In the plot below, we generate a graph of validation error against the complexity parameter of the decision tree. It is quite obvious that the validation error reaches minimum when complexity parameter equal to 0.0027. Hence we may generate another pruned decision tree for prediction with cp=0.0027.
```{r,cache=TRUE}
plotcp(DTmodf)
```
The following graph shows the structure of our pruned decision tree. Here we can see that this tree is much simplified with smaller depth.
```{r,cache=TRUE}
DTmod <- rpart(y ~ ., data = train_dat,cp=0.0027,method='class')
rpart.plot(DTmod)
```
After the decision tree model are built, we may access the feature importance by shapley value plot of the pruned decision tree. The plot following is showing the shapley value of each variable sorted by their absolute values of mean shapley values. The most influential 5 variables are interest rate index, way of contacting, number of contact in this campaign, age, and whether the previous campaign fails,.

```{r,cache=TRUE}
avgabs<-function(input){
  abs(mean(input))
}

shap_values <- fastshap::explain(
  DTmod, 
  X = train_dat,           
  feature_names = colnames(train_dat |> dplyr::select(-y)),
  pred_wrapper = pred, 
  nsim = 5,
  newdata = test_dat
)

library(ggplot2)
library(reshape2)
ggplot(melt(shap_values),aes(value, reorder(variable, value, FUN = avgabs)))+
  geom_boxplot(outlier.size=1)
```

The following plot shows the relationship between interest rate index and probability of y=yes. The curve shows overall decreasing trend,which is consistent with PDP plot in logistic regression, with a increase at interest rate index equal to negative 1. If we combine this plot with the EDA overlapped histogram in EDA, the pattern is reasonable. The EDA shows deceasing and then increasing in probability in interest rate index <-1, which is the same as what the PDP shows. When the index is larger than 0, the probability is even lower, hence we see another decreasing in the PDP plot.
```{r,cache=TRUE}
library(pdp)
pdp_dt = partial(DTmodf,pred.var = 'InterestRateindex',prob = TRUE,type = 'classification')
g <- ggplot(pdp_dt, aes(InterestRateindex, 1/(1+exp(yhat)))) +
  geom_line() +
  geom_rug(data = train_dat, aes(InterestRateindex), inherit.aes = FALSE, alpha = .5,
  color = "red") +
  theme_bw(16)
g+ylab('prob')
```

The plot following is the PDP plot of whether the person was contacted via telephone or cellular against the target variable. We can observe  that if the contact is by telephone, holding all other variable constant, the person is slightly more likely to have term deposit at the bank.This result may not be so meaningful because the shapley value plots indicates that the lower quantile, median, upper quantile contribution of this variable toward target variable is 0. In other words, decision tree does not create a meaningful boundary distinguishing these two variables. That is also why the PDP plot here shows a small difference in probability.
```{r message=FALSE, warning=FALSE,cache=TRUE}
library(pdp)
pdp_dt1 = partial(DTmodf,pred.var = 'contact',prob = TRUE,type = 'classification')
g <- ggplot(pdp_dt1, aes(contact,yhat)) +
  geom_bar(stat = 'identity',fill = 'cadetblue2') +
  geom_rug(data = train_dat, aes(contact), inherit.aes = FALSE, alpha = .5,
  color = "red") +
  theme_bw(16)
g+ylab('prob')
```

The following plot shows the relationship between number of contact in this campaign and probability of y=yes. The curve shows overall increasing trend,with a decreasing and then increasing at campaign=2.5. If we combine this plot with the EDA overlapped histogram in EDA, the pattern is reasonable. The EDA shows shows a overall increasing in proportion of y=yes with the increase in number of contact, however, the number of yes case is almost zero when campaign reaches 2.5, which explains why we observe such decrease in PDP plot.

```{r,cache=TRUE}
library(pdp)
pdp_dt2 = partial(DTmodf,pred.var = 'campaign',prob = TRUE,type = 'classification')
g <- ggplot(pdp_dt2, aes(campaign, yhat)) +
  geom_line() +
  geom_rug(data = train_dat, aes(campaign), inherit.aes = FALSE, alpha = .5,
  color = "red") +
  theme_bw(16)
g+ylab('prob')
```
The following plot shows the relationship between age and probability of y=yes. The curve shows overall increasing trend,with a decreasing and then increasing at campaign=2.5. If we combine this plot with the EDA overlapped histogram in EDA, the pattern is reasonable. The EDA shows shows a overall increasing in proportion of y=yes with the increase in number of contact, however, the number of yes case is almost zero when campaign reaches 2.5, which explains why we observe such decrease in PDP plot.

```{r,cache=TRUE}
library(pdp)
pdp_dt3 = partial(DTmodf,pred.var = 'age',prob = TRUE,type = 'classification')
g <- ggplot(pdp_dt3, aes(age, yhat)) +
  geom_line() +
  geom_rug(data = train_dat, aes(age), inherit.aes = FALSE, alpha = .5,
  color = "red") +
  theme_bw(16)
g+ylab('prob')
```

The plot following is the PDP plot of whether the previous campaign fails against the target variable. We can observe  that if previous campaign fails, holding all other variable constant, the person is slightly more likely to have term deposit at the bank.this is inconsistent with common sense. This is understandable because the shapley value plots indicates that the lower quantile, median, upper quantile contribution of this variable toward target variable is 0. In other words, decision tree does not create a meaningful boundary distinguishing these two variables. That is also why the PDP plot here shows a smaller difference in probability.
```{r message=FALSE, warning=FALSE,cache=TRUE}
library(pdp)
pdp_dt4 = partial(DTmodf,pred.var = 'poutcomefailure',prob = TRUE,type = 'classification')
g <- ggplot(pdp_dt4, aes(poutcomefailure,yhat)) +
  geom_bar(stat = 'identity',fill = 'cadetblue2') +
  geom_rug(data = train_dat, aes(poutcomefailure), inherit.aes = FALSE, alpha = .5,
  color = "red") +
  theme_bw(16)
g+ylab('prob')
```

The following cell shows the test accuracy of decision tree model. The model accuracy on test data is 0.8896359, which indicates it is a good model.

```{r,cache=TRUE}
DTpred = predict(DTmod,test_dat)[,2]
mean((DTpred>0.5)==(test_dat$y=='yes'))
```

```{r,cache=TRUE}
get_rates <- function(threshold, actual, response) {
  predicted <- ifelse(response < threshold, 0, 1)
  
  TP <- sum(actual == 1 & predicted == 1)
  FP <- sum(actual == 0 & predicted == 1)
  TN <- sum(actual == 0 & predicted == 0)
  FN <- sum(actual == 1 & predicted == 0)
  
  tpr <- TP/(TP + FN)
  fpr <- FP/(TN + FP)  
  
  data.frame(threshold, tpr, fpr)
  
}


rocdf <- map_df(seq(0, 1, .01), get_rates, test_dat$y=='yes', DTpred)
rocdf <- rocdf |> 
  mutate(label = ifelse(threshold %in% seq(0, 1, .01),
                        threshold, NA))
library(ggrepel)
g <- ggplot(rocdf, aes(x = fpr, y = tpr, label = label)) +
  geom_point() +
  geom_path() +
  geom_label_repel(color = "blue", size = 2)
plotly::ggplotly(g)
```