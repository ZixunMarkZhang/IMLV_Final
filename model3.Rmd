# [Random Forest]
```{r,cache=TRUE}
set.seed(5293)
library(randomForest)
RFmod <- randomForest(y ~ ., data = train_dat)
pred <- function(model, newdata) {
  predict(model, newdata = newdata, type = "prob")[, 2]
}
RFpred = predict(RFmod,test_dat,type = 'prob')[,2]

```
After the decision tree model are built, we may access the feature importance by shapley value plot of the random forest. The plot following is showing the shapley value of each variable sorted by their absolute values of mean shapley values. The most influential 5 variables are price index, confidence index, age, whether the previous campaign fails,and whether the person has housing loan.

```{r,cache=TRUE}
shap_values <- fastshap::explain(
  RFmod, 
  X = train_dat,           
  feature_names = colnames(train_dat |> dplyr::select(-y)),
  pred_wrapper = pred, 
  nsim = 5,
  newdata = test_dat
)
ggplot(melt(shap_values),aes(value, reorder(variable, value, FUN = avgabs)))+
  geom_boxplot()
```




```{r,cache=TRUE}
library(pdp)
pdp_rf = partial(RFmod,pred.var = 'priceIndex',prob = TRUE,type = 'classification')
g <- ggplot(pdp_rf, aes(priceIndex, yhat)) +
  geom_line() +
  geom_rug(data = train_dat, aes(priceIndex), inherit.aes = FALSE, alpha = .5,
  color = "red") +
  theme_bw(16)
g+ylab('prob')
```

```{r,cache=TRUE}
library(pdp)
pdp_rf1 = partial(RFmod,pred.var = 'confidenceIndex',prob = TRUE,type = 'classification')
g <- ggplot(pdp_rf1, aes(confidenceIndex, yhat)) +
  geom_line() +
  geom_rug(data = train_dat, aes(confidenceIndex), inherit.aes = FALSE, alpha = .5,
  color = "red") +
  theme_bw(16)
g+ylab('prob')
```
The plot following is the PDP plot of whether the person was contacted via telephone or cellular against the target variable. We can observe  that if the contact is by telephone, holding all other variable constant, the person is slightly more likely to have term deposit at the bank.this is inconsistent with what we observed in logistic regression, but same as the decision tree result. This is understandable because the shapley value plots indicates that the lower quantile, median, upper quantile contribution of this variable toward target variable is 0. In other words, random forest does not create a meaningful boundary distinguishing these two variables. That is also why the PDP plot here shows a smaller difference in probability.
```{r message=FALSE, warning=FALSE,cache=TRUE}
library(pdp)
pdp_rf2 = partial(RFmod,pred.var = 'age',prob = TRUE,type = 'classification')
g <- ggplot(pdp_rf2, aes(age,yhat)) +
  geom_line() +
  geom_rug(data = train_dat, aes(age), inherit.aes = FALSE, alpha = .5,
  color = "red") +
  theme_bw(16)
g+ylab('prob')
```
The plot following is the PDP plot of whether the previous campaign fails against the target variable. We can observe  that if previous campaign fails, holding all other variable constant, the person is slightly more likely to have term deposit at the bank.this is inconsistent with what we observed in logistic regression or common sense, but similar to the PDP plot in decision tree section. This is understandable because the shapley value plots indicates that the lower quantile, median, upper quantile contribution of this variable toward target variable is 0. In other words, random forest does not create a meaningful boundary distinguishing these two variables. That is also why the PDP plot here shows a smaller difference in probability.
```{r message=FALSE, warning=FALSE,cache=TRUE}
library(pdp)
pdp_rf3 = partial(RFmod,pred.var = 'poutcomesuccess',prob = TRUE,type = 'classification')
g <- ggplot(pdp_rf3, aes(poutcomesuccess,yhat)) +
  geom_bar(stat = 'identity',fill = 'cadetblue2') +
  geom_rug(data = train_dat, aes(poutcomesuccess), inherit.aes = FALSE, alpha = .5,
  color = "red") +
  theme_bw(16)
g+ylab('prob')
```

The plot following is the PDP plot of whether the person has housing loan against the target variable. We can observe there is no difference in probability.This is understandable because the shapley value plots indicates that the lower quantile, median, upper quantile contribution of this variable toward target variable is 0. In other words, random forest does not create a meaningful boundary distinguishing these two variables. 
```{r message=FALSE, warning=FALSE,cache=TRUE}
library(pdp)
pdp_rf4 = partial(RFmod,pred.var = 'housing',prob = TRUE,type = 'classification')
g <- ggplot(pdp_rf4, aes(housing,yhat)) +
  geom_bar(stat = 'identity',fill = 'cadetblue2') +
  geom_rug(data = train_dat, aes(housing), inherit.aes = FALSE, alpha = .5,
  color = "red") +
  theme_bw(16)
g+ylab('prob')
```
The following cell shows the test accuracy of random forest model. The model accuracy on test data is 0.8879961, which indicates it is a good model.
```{r}
mean((RFpred>0.5)==(test_dat$y=='yes'))
```


```{r,cache=TRUE}
get_rates <- function(threshold, actual, response) {
  predicted <- ifelse(response < threshold, 0, 1)
  
  TP <- sum(actual == 1 & predicted == 1)
  FP <- sum(actual == 0 & predicted == 1)
  TN <- sum(actual == 0 & predicted == 0)
  FN <- sum(actual == 1 & predicted == 0)
  
  tpr <- TP/(TP + FN)
  fpr <- FP/(TN + FP)  
  
  data.frame(threshold, tpr, fpr)
  
}


rocdf <- map_df(seq(0, 1, .01), get_rates, test_dat$y=='yes', RFpred)
rocdf <- rocdf |> 
  mutate(label = ifelse(threshold %in% seq(0, 1, .01),
                        threshold, NA))
library(ggrepel)
g <- ggplot(rocdf, aes(x = fpr, y = tpr, label = label)) +
  geom_point() +
  geom_path() +
  geom_label_repel(color = "blue", size = 2)
plotly::ggplotly(g)
```
